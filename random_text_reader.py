import os
import random
import time
import sqlite3
import numpy as np
import torch
from PIL import Image
import json
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

def tensor2pil(image):
    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))

def pil2tensor(image):
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)

def read_txt_file(file_path):
    encodings = ['utf-8', 'gbk', 'big5', 'utf-16']
    for encoding in encodings:
        try:
            with open(file_path, "r", encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    return ""

def find_image(txt_path, extensions):
    base_name = os.path.splitext(txt_path)[0]
    for ext in extensions.split(','):
        ext = ext.strip().lower()
        candidates = [
            f"{base_name}.{ext}",
            f"{base_name}_image.{ext}",
            f"{base_name}-img.{ext}",
            os.path.join(os.path.dirname(txt_path), "images", f"{os.path.basename(base_name)}.{ext}")
        ]
        for path in candidates:
            if os.path.exists(path):
                return path
    return None

def process_file_batch(file_batch_data):
    """å¤šçº¿ç¨‹å¤„ç†å•ä¸ªæ–‡ä»¶æ‰¹æ¬¡ - åªå¤„ç†æœ‰é…å¯¹å›¾ç‰‡çš„æ–‡æœ¬æ–‡ä»¶"""
    folder_path, txt_path, mod_time, image_extensions = file_batch_data
    try:
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹çš„å›¾ç‰‡
        image_path = find_image(txt_path, image_extensions)
        if not image_path:
            # æ²¡æœ‰é…å¯¹å›¾ç‰‡ï¼Œè·³è¿‡è¿™ä¸ªæ–‡ä»¶
            return None
        
        # æœ‰é…å¯¹å›¾ç‰‡ï¼Œè¯»å–æ–‡æœ¬å†…å®¹
        content = read_txt_file(txt_path)
        if not content.strip():
            # æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œä¹Ÿè·³è¿‡
            return None
            
        return (folder_path, txt_path, content, image_path, mod_time)
    except Exception as e:
        print(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {txt_path}: {str(e)}")
        return None

def background_database_builder(db_path, folder_path, image_extensions, progress_queue, stop_event, generate_missing_txt=False):
    import sys
    try:
        print("ğŸ”§ åå°æ•°æ®åº“æ„å»ºçº¿ç¨‹å·²å¯åŠ¨")
        sys.stdout.flush()
        
        # ä¸ºåå°çº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥ï¼Œä½¿ç”¨è¿æ¥æ± ä¼˜åŒ–
        conn = sqlite3.connect(db_path, check_same_thread=False, timeout=60.0)
        cursor = conn.cursor()
        
        # åå°çº¿ç¨‹ä½¿ç”¨é«˜æ€§èƒ½æ•°æ®åº“è®¾ç½®
        cursor.execute('PRAGMA journal_mode = WAL')  # WALæ¨¡å¼æ”¯æŒå¹¶å‘è¯»å†™
        cursor.execute('PRAGMA synchronous = OFF')  # å…³é—­åŒæ­¥ï¼Œæœ€å¤§åŒ–æ€§èƒ½
        cursor.execute('PRAGMA cache_size = -2048000')  # 2GBç¼“å­˜ç”¨äºåå°æ„å»º
        cursor.execute('PRAGMA temp_store = MEMORY')
        cursor.execute('PRAGMA mmap_size = 4294967296')  # 4GBå†…å­˜æ˜ å°„
        cursor.execute('PRAGMA page_size = 65536')  # 64KBé¡µé¢å¤§å°ï¼Œé€‚åˆå¤§é‡å†™å…¥
        cursor.execute('PRAGMA locking_mode = NORMAL')  # æ­£å¸¸é”æ¨¡å¼
        cursor.execute('PRAGMA busy_timeout = 60000')  # 60ç§’è¶…æ—¶
        cursor.execute('PRAGMA threads = 16')  # å¢åŠ SQLiteå†…éƒ¨çº¿ç¨‹æ•°
        cursor.execute('PRAGMA optimize')  # å¯ç”¨æŸ¥è¯¢ä¼˜åŒ–å™¨
        
        folder_path = os.path.normpath(folder_path)
        start_time = time.time()
        progress_queue.put({"type": "scan_start", "folder": folder_path})
        scan_start = time.time()
        txt_files = []
        file_mod_times = {}
        
        # è‹¥å¯ç”¨ç”Ÿæˆç¼ºå¤±æ ‡æ³¨ï¼Œå…ˆä¸ºæ²¡æœ‰åŒåtxtçš„å›¾ç‰‡åˆ›å»ºtxtï¼Œå†…å®¹ä¸ºå›¾ç‰‡æ–‡ä»¶å
        if generate_missing_txt:
            try:
                created_count = 0
                total_images = 0
                ext_list = [ext.strip().lower() for ext in image_extensions.split(',') if ext.strip()]
                for root, _, files in os.walk(folder_path):
                    if stop_event.is_set():
                        conn.close()
                        return
                    for f in files:
                        lf = f.lower()
                        if any(lf.endswith(f'.{ext}') for ext in ext_list):
                            total_images += 1
                            image_path = os.path.join(root, f)
                            base, _ = os.path.splitext(image_path)
                            txt_path = f"{base}.txt"
                            if not os.path.exists(txt_path):
                                try:
                                    with open(txt_path, 'w', encoding='utf-8') as tf:
                                        tf.write(os.path.basename(base))
                                    created_count += 1
                                except Exception as e:
                                    print(f"âš ï¸ åˆ›å»ºæ ‡æ³¨æ–‡ä»¶å¤±è´¥ [{txt_path}]: {str(e)}")
                progress_queue.put({
                    "type": "txt_generation",
                    "created": created_count,
                    "total_images": total_images
                })
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆç¼ºå¤±æ ‡æ³¨æ—¶å‡ºé”™: {str(e)}")
        
        # æ–‡ä»¶æ‰«æé˜¶æ®µ
        for root, _, files in os.walk(folder_path):
            if stop_event.is_set():
                conn.close()
                return
            for f in files:
                if f.lower().endswith(".txt"):
                    txt_path = os.path.join(root, f)
                    mod_time = os.path.getmtime(txt_path)
                    txt_files.append(txt_path)
                    file_mod_times[txt_path] = mod_time
        
        scan_time = time.time() - scan_start
        progress_queue.put({
            "type": "scan_complete", 
            "count": len(txt_files), 
            "time": scan_time
        })
        
        if not txt_files or stop_event.is_set():
            conn.close()
            return
        
        # æ•°æ®åº“æŸ¥è¯¢é˜¶æ®µ - ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
        progress_queue.put({"type": "query_start"})
        query_start = time.time()
        db_files = {}
        query_batch_size = 50000  # å¢åŠ æ‰¹æ¬¡å¤§å°
        
        for i in range(0, len(txt_files), query_batch_size):
            if stop_event.is_set():
                conn.close()
                return
            batch_files = txt_files[i:i + query_batch_size]
            placeholders = ','.join(['?'] * len(batch_files))
            cursor.execute(f"""
                SELECT txt_path, last_modified 
                FROM file_cache 
                WHERE txt_path IN ({placeholders})
            """, batch_files)
            batch_results = dict(cursor.fetchall())
            db_files.update(batch_results)
            
            if len(txt_files) > query_batch_size:
                progress_queue.put({
                    "type": "query_progress",
                    "current": min(i + query_batch_size, len(txt_files)),
                    "total": len(txt_files)
                })
        
        query_time = time.time() - query_start
        progress_queue.put({"type": "query_complete", "time": query_time})
        
        # åˆ†æéœ€è¦å¤„ç†çš„æ–‡ä»¶
        files_to_process = []
        for txt_path in txt_files:
            if stop_event.is_set():
                conn.close()
                return
            current_mod_time = file_mod_times[txt_path]
            db_mod_time = db_files.get(txt_path)
            if db_mod_time is None or db_mod_time != current_mod_time:
                files_to_process.append((txt_path, current_mod_time))
        
        existing_files = len(txt_files) - len(files_to_process)
        progress_queue.put({
            "type": "analysis_complete",
            "existing": existing_files,
            "new_or_modified": len(files_to_process)
        })
        
        if not files_to_process or stop_event.is_set():
            conn.close()
            return
        
        # å¤šçº¿ç¨‹æ–‡ä»¶å¤„ç†é˜¶æ®µ
        progress_queue.put({"type": "process_start", "total": len(files_to_process)})
        process_start = time.time()
        processed_count = 0
        skipped_count = 0  # è·³è¿‡çš„æ–‡ä»¶æ•°é‡
        
        # åŠ¨æ€ç¡®å®šçº¿ç¨‹æ•°ï¼ŒåŸºäºCPUæ ¸å¿ƒæ•°å’Œæ–‡ä»¶æ•°é‡
        cpu_count = multiprocessing.cpu_count()
        max_workers = min(max(cpu_count * 2, 8), 32)  # æœ€å°‘8ä¸ªï¼Œæœ€å¤š32ä¸ªçº¿ç¨‹
        file_processing_batch_size = 100  # æ–‡ä»¶å¤„ç†æ‰¹æ¬¡å¤§å°
        db_batch_size = 1000  # æ•°æ®åº“æ‰¹æ¬¡å¤§å°
        
        print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼ˆåªå¤„ç†å›¾æ–‡é…å¯¹æ–‡ä»¶ï¼‰")
        
        # å‡†å¤‡æ–‡ä»¶å¤„ç†æ•°æ®
        file_batch_data = [
            (folder_path, txt_path, mod_time, image_extensions)
            for txt_path, mod_time in files_to_process
        ]
        
        batch_data = []
        last_progress_time = process_start
        last_progress_count = 0  # è®°å½•ä¸Šæ¬¡æ‰“å°è¿›åº¦æ—¶çš„å¤„ç†æ•°é‡
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ†æ‰¹æäº¤ä»»åŠ¡ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
            for i in range(0, len(file_batch_data), file_processing_batch_size):
                if stop_event.is_set():
                    break
                
                batch_files = file_batch_data[i:i + file_processing_batch_size]
                future_to_file = {
                    executor.submit(process_file_batch, file_data): file_data 
                    for file_data in batch_files
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    if stop_event.is_set():
                        break
                    
                    result = future.result()
                    if result:
                        batch_data.append(result)
                        processed_count += 1
                    else:
                        skipped_count += 1
                    
                    # å½“ç§¯ç´¯è¶³å¤Ÿæ•°æ®æˆ–å¤„ç†å®Œä¸€æ‰¹æ—¶ï¼Œå†™å…¥æ•°æ®åº“
                    if len(batch_data) >= db_batch_size or (processed_count + skipped_count) % file_processing_batch_size == 0:
                        if batch_data:
                            # ä½¿ç”¨äº‹åŠ¡å¿«é€Ÿå†™å…¥
                            cursor.execute('BEGIN IMMEDIATE')
                            try:
                                cursor.executemany('''
                                    INSERT OR REPLACE INTO file_cache 
                                    (folder_path, txt_path, content, image_path, last_modified)
                                    VALUES (?, ?, ?, ?, ?)
                                ''', batch_data)
                                cursor.execute('COMMIT')
                            except Exception as e:
                                cursor.execute('ROLLBACK')
                                raise e
                            
                            batch_data = []
                        
                        # ä¼˜åŒ–è¿›åº¦æ›´æ–°ï¼šå‡å°‘æ‰“å°é¢‘ç‡
                        current_time = time.time()
                        elapsed_since_last = current_time - last_progress_time
                        total_elapsed = current_time - process_start
                        total_processed_files = processed_count + skipped_count
                        current_progress_percent = total_processed_files * 100 // len(files_to_process)
                        last_progress_percent = (last_progress_count) * 100 // len(files_to_process)
                        
                        # åªåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰“å°è¿›åº¦ï¼š
                        should_print_progress = (
                            elapsed_since_last >= 15.0 or  # è‡³å°‘15ç§’é—´éš”
                            total_processed_files % 50000 == 0 or  # æ¯5ä¸‡ä¸ªæ–‡ä»¶
                            current_progress_percent >= last_progress_percent + 5  # æ¯5%è¿›åº¦å˜åŒ–
                        )
                        
                        if should_print_progress:
                            files_processed_since_last = total_processed_files - last_progress_count
                            speed = files_processed_since_last / elapsed_since_last if elapsed_since_last > 0 else 0
                            
                            cursor.execute("SELECT COUNT(*) FROM file_cache")
                            current_total_files = cursor.fetchone()[0]
                            
                            progress_queue.put({
                                "type": "process_progress",
                                "processed": processed_count,
                                "skipped": skipped_count,
                                "total": len(files_to_process),
                                "speed": speed,
                                "elapsed": total_elapsed,
                                "db_total": current_total_files,
                                "progress_percent": current_progress_percent
                            })
                            last_progress_time = current_time
                            last_progress_count = total_processed_files
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if batch_data and not stop_event.is_set():
            cursor.execute('BEGIN IMMEDIATE')
            try:
                cursor.executemany('''
                    INSERT OR REPLACE INTO file_cache 
                    (folder_path, txt_path, content, image_path, last_modified)
                    VALUES (?, ?, ?, ?, ?)
                ''', batch_data)
                cursor.execute('COMMIT')
            except Exception as e:
                cursor.execute('ROLLBACK')
                raise e
        
        # ä¼˜åŒ–æ•°æ®åº“
        cursor.execute('PRAGMA optimize')
        cursor.execute('VACUUM')  # æ¸…ç†æ•°æ®åº“æ–‡ä»¶
        
        total_time = time.time() - start_time
        process_time = time.time() - process_start
        avg_speed = (processed_count + skipped_count) / process_time if process_time > 0 else 0
        
        cursor.execute("SELECT COUNT(*) FROM file_cache")
        final_total = cursor.fetchone()[0]
        
        progress_queue.put({
            "type": "complete",
            "processed": processed_count,
            "skipped": skipped_count,
            "avg_speed": avg_speed,
            "total_time": total_time,
            "db_total": final_total
        })
        
        conn.close()
        
    except Exception as e:
        progress_queue.put({"type": "error", "message": str(e)})

class TextWithImageReader:
    def __init__(self):
        self.history = []
        self.blank_image = self.create_blank_image()
        
        # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©ä¸åŒçš„æ•°æ®åº“æ–‡ä»¶å
        import platform
        system = platform.system().lower()
        if system == "linux":
            db_filename = "text_image_reader_cache_linux.db"
        else:
            db_filename = "text_image_reader_cache.db"
        
        self.db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), db_filename)
        self.conn = None
        self.cursor = None
        self.last_folder = ""
        self.background_process = None
        self.stop_event = threading.Event()
        self.progress_queue = queue.Queue()
        self.progress_thread = None
        self.is_building = False
        # æ™ºèƒ½æœç´¢ç»“æœç¼“å­˜
        self.last_search_conditions = None
        self.last_search_results = None
        # è¿æ¥æ± é”
        self.db_lock = threading.RLock()
        
        # é¡ºåºè¯»å–ç›¸å…³å±æ€§
        self.sequential_mode = False
        self.sequential_results = []
        self.sequential_index = 0
        self.sequential_folder = ""
        self.sequential_conditions = None
        
        print(f"ğŸ”§ åˆå§‹åŒ–æ•°æ®åº“: {os.path.basename(self.db_path)}")
        self.initialize_database()
    
    def initialize_database(self):
        # ä¸»çº¿ç¨‹æ•°æ®åº“è¿æ¥ï¼Œä¼˜åŒ–ä¸ºé«˜æ€§èƒ½è¯»å–
        with self.db_lock:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=60.0)
            self.cursor = self.conn.cursor()
            
            # ä¸»çº¿ç¨‹ä½¿ç”¨è¯»ä¼˜åŒ–é…ç½®
            self.cursor.execute('PRAGMA journal_mode = WAL')  # WALæ¨¡å¼æ”¯æŒå¹¶å‘è¯»å†™
            self.cursor.execute('PRAGMA synchronous = NORMAL')  # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§
            self.cursor.execute('PRAGMA cache_size = -3072000')  # 3GBç¼“å­˜ç”¨äºä¸»çº¿ç¨‹è¯»å–
            self.cursor.execute('PRAGMA temp_store = MEMORY')  # ä¸´æ—¶è¡¨å…¨éƒ¨å†…å­˜
            self.cursor.execute('PRAGMA mmap_size = 6442450944')  # 6GBå†…å­˜æ˜ å°„
            self.cursor.execute('PRAGMA page_size = 4096')  # æ ‡å‡†é¡µé¢å¤§å°
            self.cursor.execute('PRAGMA locking_mode = NORMAL')  # æ­£å¸¸é”æ¨¡å¼ï¼Œå…è®¸å¹¶å‘
            self.cursor.execute('PRAGMA busy_timeout = 60000')  # 60ç§’è¶…æ—¶
            self.cursor.execute('PRAGMA count_changes = OFF')  # å…³é—­è®¡æ•°
            self.cursor.execute('PRAGMA auto_vacuum = NONE')  # å…³é—­è‡ªåŠ¨æ¸…ç†
            self.cursor.execute('PRAGMA threads = 16')  # ä¸»çº¿ç¨‹ä½¿ç”¨æ›´å¤šçº¿ç¨‹
            self.cursor.execute('PRAGMA optimize')  # å¯ç”¨æŸ¥è¯¢ä¼˜åŒ–å™¨
            
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_cache (
                    id INTEGER PRIMARY KEY,
                    folder_path TEXT NOT NULL,
                    txt_path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    image_path TEXT,
                    last_modified REAL NOT NULL
                )
            ''')
            
            # é«˜æ€§èƒ½ç´¢å¼•ç­–ç•¥ - æ·»åŠ æ›´å¤šå¤åˆç´¢å¼•
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_folder_fast ON file_cache(folder_path) WHERE folder_path IS NOT NULL')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_folder_content_fast ON file_cache(folder_path, content) WHERE content IS NOT NULL')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_fast ON file_cache(content) WHERE content IS NOT NULL')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_txt_path_fast ON file_cache(txt_path)')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_last_modified ON file_cache(last_modified)')
            
            self.conn.commit()
    
    def create_blank_image(self):
        blank = Image.new("RGBA", (512, 512), (0, 0, 0, 0))
        blank = blank.convert("RGB")
        blank_array = np.array(blank).astype(np.float32) / 255.0
        return torch.from_numpy(blank_array)[None,]
    
    def start_background_building(self, folder_path, image_extensions, generate_missing_txt=False):
        self.stop_background_building()
        self.stop_event = threading.Event()
        self.progress_queue = queue.Queue()
        self.background_process = threading.Thread(
            target=background_database_builder,
            args=(self.db_path, folder_path, image_extensions, self.progress_queue, self.stop_event, generate_missing_txt)
        )
        self.background_process.daemon = True
        self.background_process.start()
        self.progress_thread = threading.Thread(target=self.monitor_progress)
        self.progress_thread.daemon = True
        self.progress_thread.start()
        self.is_building = True
        print(f"ğŸš€ å·²å¯åŠ¨åå°æ•°æ®åº“æ„å»ºçº¿ç¨‹ (æ•°æ®åº“: {os.path.basename(self.db_path)})")
    
    def stop_background_building(self):
        if self.background_process and self.background_process.is_alive():
            print("â¹ï¸ åœæ­¢åå°æ•°æ®åº“æ„å»ºçº¿ç¨‹...")
            self.stop_event.set()
            self.background_process.join(timeout=3)
            print("âœ… åå°çº¿ç¨‹å·²åœæ­¢")
        self.is_building = False
        self.background_process = None
    
    def monitor_progress(self):
        import sys
        print("ğŸ”§ è¿›åº¦ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
        sys.stdout.flush()
        while self.is_building:
            try:
                progress = self.progress_queue.get(timeout=1)
                if progress["type"] == "scan_start":
                    print(f"ğŸ“ åå°æ‰«ææ–‡ä»¶å¤¹: {progress['folder']}")
                    sys.stdout.flush()
                elif progress["type"] == "scan_complete":
                    print(f"ğŸ“Š è¾“å…¥æ–‡ä»¶å¤¹å…±æ‰¾åˆ° {progress['count']} ä¸ªæ–‡æœ¬æ–‡ä»¶ (è€—æ—¶: {progress['time']:.2f}ç§’)")
                    sys.stdout.flush()
                elif progress["type"] == "query_start":
                    print("ğŸ” æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡ä»¶çŠ¶æ€...")
                    sys.stdout.flush()
                elif progress["type"] == "query_progress":
                    print(f"ğŸ” å·²æ£€æŸ¥ {progress['current']}/{progress['total']} ä¸ªæ–‡ä»¶...")
                    sys.stdout.flush()
                elif progress["type"] == "query_complete":
                    print(f"ğŸ” æ•°æ®åº“æŸ¥è¯¢å®Œæˆ (è€—æ—¶: {progress['time']:.2f}ç§’)")
                    sys.stdout.flush()
                elif progress["type"] == "txt_generation":
                    print(f"ğŸ“ å·²ä¸º {progress.get('total_images', 0)} å¼ å›¾ç‰‡ç”Ÿæˆç¼ºå¤±æ ‡æ³¨ä¸­çš„ {progress.get('created', 0)} ä¸ª")
                    sys.stdout.flush()
                elif progress["type"] == "analysis_complete":
                    print(f"ğŸ“Š æ–‡ä»¶åˆ†æ:")
                    print(f"   - å·²å­˜åœ¨ä¸”æœªä¿®æ”¹: {progress['existing']} ä¸ªæ–‡ä»¶")
                    print(f"   - æ–°å¢æˆ–å·²ä¿®æ”¹: {progress['new_or_modified']} ä¸ªæ–‡ä»¶")
                    sys.stdout.flush()
                elif progress["type"] == "process_start":
                    print(f"ğŸ”„ éœ€è¦å¤„ç† {progress['total']} ä¸ªæ–‡ä»¶")
                    sys.stdout.flush()
                elif progress["type"] == "process_progress":
                    progress_percent = progress.get('progress_percent', 0)
                    processed = progress['processed']
                    skipped = progress.get('skipped', 0)
                    total = progress['total']
                    speed = progress['speed']
                    elapsed = progress['elapsed']
                    db_total = progress['db_total']
                    
                    # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
                    total_processed = processed + skipped
                    if speed > 0:
                        remaining_files = total - total_processed
                        eta_seconds = remaining_files / speed
                        eta_minutes = eta_seconds / 60
                        if eta_minutes < 1:
                            eta_str = f"{eta_seconds:.0f}ç§’"
                        elif eta_minutes < 60:
                            eta_str = f"{eta_minutes:.1f}åˆ†é’Ÿ"
                        else:
                            eta_hours = eta_minutes / 60
                            eta_str = f"{eta_hours:.1f}å°æ—¶"
                    else:
                        eta_str = "è®¡ç®—ä¸­..."
                    
                    print(f"â³ æ•°æ®åº“æ„å»ºè¿›åº¦: {progress_percent}% ({total_processed:,}/{total:,}) | "
                          f"å›¾æ–‡å¯¹: {processed:,} | è·³è¿‡: {skipped:,} | "
                          f"é€Ÿåº¦: {speed:.0f}æ–‡ä»¶/ç§’ | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ | "
                          f"é¢„è®¡å‰©ä½™: {eta_str} | æ•°æ®åº“æ€»è®¡: {db_total:,}")
                    sys.stdout.flush()
                elif progress["type"] == "complete":
                    processed = progress['processed']
                    skipped = progress.get('skipped', 0)
                    total_files = processed + skipped
                    print(f"âœ… åå°æ„å»ºå®Œæˆï¼å…±å¤„ç†äº† {total_files:,} ä¸ªæ–‡ä»¶")
                    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ: å›¾æ–‡å¯¹ {processed:,} ä¸ªï¼Œè·³è¿‡ {skipped:,} ä¸ªï¼ˆæ— é…å¯¹å›¾ç‰‡æˆ–å†…å®¹ä¸ºç©ºï¼‰")
                    print(f"ğŸ“ˆ å¤„ç†é€Ÿåº¦: {progress['avg_speed']:.0f}æ–‡ä»¶/ç§’, æ€»è€—æ—¶: {progress['total_time']:.2f}ç§’")
                    print(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {progress['db_total']:,} ä¸ªå›¾æ–‡å¯¹ç¼“å­˜")
                    sys.stdout.flush()
                    self.is_building = False
                    # æ„å»ºå®Œæˆåæ¸…ç©ºæœç´¢ç¼“å­˜ï¼Œç¡®ä¿ç”¨æˆ·èƒ½è·å–åˆ°æœ€æ–°æ•°æ®
                    self.clear_search_cache("æ•°æ®åº“æ„å»ºå®Œæˆ")
                elif progress["type"] == "error":
                    print(f"âŒ åå°æ„å»ºå‡ºé”™: {progress['message']}")
                    sys.stdout.flush()
                    self.is_building = False
                    # æ„å»ºå‡ºé”™æ—¶ä¹Ÿæ¸…ç©ºæœç´¢ç¼“å­˜
                    self.clear_search_cache("æ•°æ®åº“æ„å»ºå‡ºé”™")
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âš ï¸ è¿›åº¦ç›‘æ§å‡ºé”™: {str(e)}")
                sys.stdout.flush()
                break
        print("ğŸ”§ è¿›åº¦ç›‘æ§çº¿ç¨‹å·²ç»“æŸ")
        sys.stdout.flush()
    
    def ensure_database_connection(self):
        """ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨ï¼Œå¦‚æœè¿æ¥æ–­å¼€åˆ™é‡æ–°è¿æ¥"""
        try:
            with self.db_lock:
                # æµ‹è¯•è¿æ¥æ˜¯å¦å¯ç”¨
                self.cursor.execute("SELECT 1")
                return True
        except (sqlite3.OperationalError, sqlite3.DatabaseError, AttributeError):
            print("ğŸ”§ æ•°æ®åº“è¿æ¥å·²æ–­å¼€ï¼Œæ­£åœ¨é‡æ–°è¿æ¥...")
            try:
                if self.conn:
                    self.conn.close()
            except:
                pass
            self.initialize_database()
            print("âœ… æ•°æ®åº“è¿æ¥å·²æ¢å¤")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def fast_query(self, folder_path, keywords, negative_keywords, strict_match, search_entire_database, history_paths):
        """é«˜é€ŸæŸ¥è¯¢æ–¹æ³• - ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“è®¿é—®"""
        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not self.ensure_database_connection():
            print("âŒ æ— æ³•å»ºç«‹æ•°æ®åº“è¿æ¥")
            return []
            
        max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
        retry_delay = 0.05  # å‡å°‘åˆå§‹å»¶è¿Ÿ
        
        for attempt in range(max_retries):
            try:
                with self.db_lock:  # ä½¿ç”¨é”ä¿æŠ¤æ•°æ®åº“è®¿é—®
                    query_params = []
                    where_clauses = []
                    
                    # æ–‡ä»¶å¤¹æ¡ä»¶
                    if not search_entire_database and folder_path:
                        where_clauses.append("folder_path = ?")
                        query_params.append(folder_path)
                    
                    # å†å²è®°å½•è¿‡æ»¤
                    if history_paths:
                        placeholders = ','.join(['?'] * len(history_paths))
                        where_clauses.append(f"txt_path NOT IN ({placeholders})")
                        query_params.extend(history_paths)
                    
                    # å…³é”®è¯åŒ¹é…
                    keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
                    if keyword_list:
                        if strict_match:
                            # ä¸¥æ ¼åŒ¹é…é€»è¾‘
                            keyword_conditions = []
                            for keyword in keyword_list:
                                keyword_lower = keyword.lower()
                                
                                if (keyword_lower.startswith('"') and keyword_lower.endswith('"')) or \
                                   (keyword_lower.startswith("'") and keyword_lower.endswith("'")):
                                    # å®Œæ•´çŸ­è¯­åŒ¹é…
                                    phrase = keyword_lower[1:-1]
                                    keyword_conditions.append("LOWER(content) LIKE ?")
                                    query_params.append(f"%{phrase}%")
                                    print(f"ğŸ¯ ä¸¥æ ¼åŒ¹é…çŸ­è¯­: '{phrase}'")
                                else:
                                    # å•è¯è¾¹ç•ŒåŒ¹é…
                                    keyword_conditions.append("""
                                        (LOWER(content) LIKE ? OR 
                                         LOWER(content) LIKE ? OR 
                                         LOWER(content) LIKE ? OR 
                                         LOWER(content) LIKE ?)
                                    """.strip())
                                    query_params.extend([
                                        f"{keyword_lower} %",
                                        f"% {keyword_lower}",
                                        f"% {keyword_lower} %",
                                        keyword_lower
                                    ])
                                    print(f"ğŸ¯ ä¸¥æ ¼åŒ¹é…å•è¯: '{keyword_lower}' (å•è¯è¾¹ç•Œ)")
                            
                            where_clauses.append(f"({' AND '.join(keyword_conditions)})")
                        else:
                            # å®½æ¾åŒ¹é…
                            keyword_conditions = []
                            for keyword in keyword_list:
                                keyword_conditions.append("LOWER(content) LIKE ?")
                                query_params.append(f"%{keyword.lower()}%")
                            where_clauses.append(f"({' OR '.join(keyword_conditions)})")
                    
                    # å¦å®šå…³é”®è¯
                    negative_keyword_list = [kw.strip().lower() for kw in negative_keywords.split(",") if kw.strip()]
                    if negative_keyword_list:
                        for neg_keyword in negative_keyword_list:
                            where_clauses.append("LOWER(content) NOT LIKE ?")
                            query_params.append(f"%{neg_keyword}%")
                    
                    where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
                    
                    # æ‰§è¡ŒæŸ¥è¯¢
                    query = f"SELECT txt_path, content, image_path FROM file_cache WHERE {where_clause}"
                    self.cursor.execute(query, query_params)
                    results = self.cursor.fetchall()
                    
                    return results
                
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e).lower() and attempt < max_retries - 1:
                    print(f"âš ï¸ æ•°æ®åº“è¢«é”å®šï¼Œ{retry_delay}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 2.0)  # æ›´æ¸©å’Œçš„æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
                    raise e
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                raise e
        
        return []

    def get_search_fingerprint(self, folder_path, keywords, negative_keywords, strict_match, search_entire_database):
        """ç”Ÿæˆæœç´¢æ¡ä»¶çš„æŒ‡çº¹ï¼Œç”¨äºåˆ¤æ–­æœç´¢æ¡ä»¶æ˜¯å¦å˜åŒ–"""
        return f"{folder_path}|{keywords}|{negative_keywords}|{strict_match}|{search_entire_database}"
    
    def should_use_cache(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æœç´¢ç¼“å­˜ - åªæœ‰åœ¨æ•°æ®åº“ç¨³å®šæ—¶æ‰ä½¿ç”¨ç¼“å­˜"""
        # å¦‚æœæ­£åœ¨æ„å»ºæ•°æ®åº“ï¼Œä¸ä½¿ç”¨ç¼“å­˜
        if self.is_building:
            return False
        
        # å¦‚æœç¼“å­˜ç»“æœä¸ºç©ºä¸”ä¹‹å‰æ²¡æœ‰æœç´¢è¿‡ï¼Œå¯èƒ½æ˜¯æ•°æ®åº“è¿˜åœ¨æ„å»ºæ—¶ç¼“å­˜çš„ç©ºç»“æœ
        if (self.last_search_results is not None and 
            len(self.last_search_results) == 0 and 
            self.last_search_conditions is not None):
            # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦ç°åœ¨æœ‰æ•°æ®äº†
            try:
                with self.db_lock:
                    self.cursor.execute("SELECT COUNT(*) FROM file_cache LIMIT 1")
                    count = self.cursor.fetchone()[0]
                    if count > 0:
                        # æ•°æ®åº“ç°åœ¨æœ‰æ•°æ®äº†ï¼Œæ¸…ç©ºä¹‹å‰çš„ç©ºç¼“å­˜
                        print("ğŸ”„ æ£€æµ‹åˆ°æ•°æ®åº“ç°åœ¨æœ‰æ•°æ®ï¼Œæ¸…ç©ºä¹‹å‰çš„ç©ºç¼“å­˜")
                        self.last_search_conditions = None
                        self.last_search_results = None
                        return False
            except Exception as e:
                print(f"âš ï¸ æ£€æŸ¥æ•°æ®åº“çŠ¶æ€å¤±è´¥: {str(e)}")
                return False
        
        return True
    
    def clear_search_cache(self, reason=""):
        """æ¸…ç©ºæœç´¢ç¼“å­˜"""
        if reason:
            print(f"ğŸ”„ æ¸…ç©ºæœç´¢ç¼“å­˜: {reason}")
        self.last_search_conditions = None
        self.last_search_results = None
        
        # åŒæ—¶æ¸…ç©ºé¡ºåºè¯»å–ç¼“å­˜
        if self.sequential_mode:
            self.reset_sequential_mode("æœç´¢ç¼“å­˜æ¸…ç†")

    def reset_sequential_mode(self, reason=""):
        """é‡ç½®é¡ºåºè¯»å–æ¨¡å¼"""
        if reason:
            print(f"ğŸ”„ é‡ç½®é¡ºåºè¯»å–æ¨¡å¼: {reason}")
        self.sequential_mode = False
        self.sequential_results = []
        self.sequential_index = 0
        self.sequential_folder = ""
        self.sequential_conditions = None

    def should_reset_sequential_mode(self, folder_path, keywords, negative_keywords, strict_match, search_entire_database):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®é¡ºåºè¯»å–æ¨¡å¼"""
        current_conditions = self.get_search_fingerprint(folder_path, keywords, negative_keywords, strict_match, search_entire_database)
        
        # å¦‚æœæœç´¢æ¡ä»¶å˜åŒ–ï¼Œéœ€è¦é‡ç½®
        if self.sequential_conditions != current_conditions:
            return True
        
        # å¦‚æœæ–‡ä»¶å¤¹å˜åŒ–ï¼Œéœ€è¦é‡ç½®
        if not search_entire_database and self.sequential_folder != folder_path:
            return True
        
        # å¦‚æœä»æ•´ä¸ªæ•°æ®åº“æœç´¢å˜ä¸ºæ–‡ä»¶å¤¹æœç´¢ï¼Œéœ€è¦é‡ç½®
        if self.sequential_mode and search_entire_database != (self.sequential_folder == ""):
            return True
        
        return False

    def get_sequential_result(self):
        """è·å–ä¸‹ä¸€ä¸ªé¡ºåºç»“æœ"""
        if not self.sequential_results or self.sequential_index >= len(self.sequential_results):
            return None
        
        result = self.sequential_results[self.sequential_index]
        self.sequential_index += 1
        
        # å¦‚æœåˆ°è¾¾åˆ—è¡¨æœ«å°¾ï¼Œå¾ªç¯å›åˆ°å¼€å§‹
        if self.sequential_index >= len(self.sequential_results):
            self.sequential_index = 0
            print(f"ğŸ”„ é¡ºåºè¯»å–å·²åˆ°è¾¾æœ«å°¾ï¼Œé‡æ–°å¼€å§‹å¾ªç¯ (å…± {len(self.sequential_results)} ä¸ªç»“æœ)")
        
        return result

    def ultra_fast_select(self, results, history_set, max_attempts=100):
        """è¶…é«˜é€Ÿé€‰æ‹© - ä¼˜åŒ–éšæœºé€‰æ‹©ç®—æ³•"""
        if not results:
            return None
        
        if not history_set:
            # æ²¡æœ‰å†å²è®°å½•é™åˆ¶ï¼Œç›´æ¥éšæœºé€‰æ‹©
            import random
            return random.choice(results)
        
        # æœ‰å†å²è®°å½•é™åˆ¶ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„é€‰æ‹©ç®—æ³•
        total_results = len(results)
        
        # å¦‚æœå†å²è®°å½•å¾ˆå°‘ï¼Œä½¿ç”¨éšæœºå°è¯•
        if len(history_set) < total_results * 0.8:
            for _ in range(max_attempts):
                import random
                selected = random.choice(results)
                if selected[0] not in history_set:
                    return selected
        
        # å¦‚æœå†å²è®°å½•è¾ƒå¤šï¼Œç›´æ¥è¿‡æ»¤
        print("âš¡ ä½¿ç”¨é¢„è¿‡æ»¤æ¨¡å¼é€‰æ‹©ç»“æœ")
        available = [result for result in results if result[0] not in history_set]
        if available:
            import random
            return random.choice(available)
        
        return None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "keywords": ("STRING", {"default": ""}),
                "negative_keywords": ("STRING", {"default": ""}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
                "history_depth": ("INT", {"default": 3, "min": 1, "max": 100}),
                "image_extensions": ("STRING", {"default": "png,jpg,jpeg,webp"}),
            },
            "optional": {
                "folder_path": ("STRING", {"default": ""}),
                "strict_match": ("BOOLEAN", {"default": False}),
                "force_cache_update": ("BOOLEAN", {"default": False}),
                "search_entire_database": ("BOOLEAN", {"default": False}),
                "sequential_read": ("BOOLEAN", {"default": False}),
                "generate_missing_txt": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE")
    FUNCTION = "process"
    CATEGORY = "hhy"

    def process(self, keywords, negative_keywords, seed, history_depth, image_extensions, 
                folder_path="", strict_match=False, force_cache_update=False, search_entire_database=False, sequential_read=False, generate_missing_txt=False):
        
        # ä¸¥æ ¼åŒ¹é…ä½¿ç”¨è¯´æ˜
        if strict_match and keywords:
            print("ğŸ¯ ä¸¥æ ¼åŒ¹é…æ¨¡å¼å·²å¯ç”¨ï¼")
            print("ğŸ’¡ ä½¿ç”¨æç¤º:")
            print("   - æ™®é€šå…³é”®è¯: ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é… (ä¾‹å¦‚: white,panty)")
            print("   - å®Œæ•´çŸ­è¯­: ç”¨å¼•å·åŒ…å›´ (ä¾‹å¦‚: \"white panty\",\"black dress\")")
            print("   - å¤šä¸ªæ¡ä»¶: ç”¨é€—å·åˆ†éš”ï¼Œæ‰€æœ‰æ¡ä»¶éƒ½å¿…é¡»æ»¡è¶³ (ANDå…³ç³»)")
            print("   - ç¤ºä¾‹: 'white,panty' è¦æ±‚åŒæ—¶åŒ…å«è¿™ä¸¤ä¸ªå®Œæ•´å•è¯")
            print("   - ç¤ºä¾‹: '\"white panty\"' è¦æ±‚åŒ…å«å®Œæ•´çŸ­è¯­ 'white panty'")
        
        # é¡ºåºè¯»å–æ¨¡å¼è¯´æ˜
        if sequential_read:
            print("ğŸ“– é¡ºåºè¯»å–æ¨¡å¼å·²å¯ç”¨ï¼")
            print("ğŸ’¡ é¡ºåºè¯»å–ç‰¹æ€§:")
            print("   - ä¸å—ç§å­å’Œå†å²è®°å½•å½±å“")
            print("   - æŒ‰å›ºå®šé¡ºåºä¾æ¬¡è¾“å‡ºç»“æœ")
            print("   - åˆ°è¾¾æœ«å°¾åè‡ªåŠ¨å¾ªç¯")
            print("   - æœç´¢æ¡ä»¶å˜åŒ–æ—¶è‡ªåŠ¨é‡ç½®")
        
        # æ£€æŸ¥folder_pathæ˜¯å¦ä¸ºç©º
        if not folder_path or not folder_path.strip():
            print("ğŸ“Œ æœªæŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå°†ä»æ•´ä¸ªæ•°æ®åº“ä¸­æœç´¢æ•°æ®")
            print("ğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦æ›´æ–°ç‰¹å®šæ–‡ä»¶å¤¹çš„æ•°æ®ï¼Œè¯·æŒ‡å®šfolder_pathå‚æ•°")
            
            # çº¿ç¨‹å®‰å…¨åœ°æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©º
            with self.db_lock:
                self.cursor.execute("SELECT COUNT(*) FROM file_cache")
                total_count = self.cursor.fetchone()[0]
            
            if total_count == 0:
                error_msg = ("âŒ æ•°æ®åº“ä¸ºç©ºï¼è¯·å…ˆæŒ‡å®š folder_path å‚æ•°æ¥æ‰«ææ–‡ä»¶å¤¹å¹¶æ„å»ºæ•°æ®åº“ç¼“å­˜ã€‚\n"
                           "ç¤ºä¾‹ï¼šè®¾ç½® folder_path ä¸ºåŒ…å«æ–‡æœ¬æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„")
                print(error_msg)
                return (error_msg, self.blank_image)
            
            print(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {total_count} ä¸ªæ–‡ä»¶ç¼“å­˜å¯ä¾›æœç´¢")
            
            # åœæ­¢ä»»ä½•æ­£åœ¨è¿›è¡Œçš„åå°æ„å»ºä»»åŠ¡
            if self.is_building:
                print("â¹ï¸ åœæ­¢å½“å‰æ­£åœ¨è¿›è¡Œçš„æ•°æ®åº“æ„å»ºä»»åŠ¡...")
                self.stop_background_building()
            
            # é‡ç½®last_folderï¼Œç¡®ä¿ä¸‹æ¬¡è¾“å…¥æ–‡ä»¶å¤¹æ—¶ä¼šé‡æ–°æ„å»º
            self.last_folder = ""
            
            # å¼ºåˆ¶è®¾ç½®ä¸ºæœç´¢æ•´ä¸ªæ•°æ®åº“
            search_entire_database = True
            folder_path = ""  # ç¡®ä¿folder_pathä¸ºç©ºå­—ç¬¦ä¸²
        else:
            folder_path = os.path.normpath(folder_path)
        
        if not search_entire_database and not os.path.exists(folder_path):
            error_msg = f"âŒ è·¯å¾„ä¸å­˜åœ¨: {folder_path}"
            print(error_msg)
            return (error_msg, self.blank_image)
        
        # é¡ºåºè¯»å–æ¨¡å¼ä¸‹ä¸è®¾ç½®ç§å­
        if not sequential_read:
            torch.manual_seed(seed)
        
        # æ£€æŸ¥æ˜¯å¦æ›´æ¢äº†æ–‡ä»¶å¤¹ï¼Œå¦‚æœæ˜¯åˆ™æ¸…ç©ºæœç´¢ç¼“å­˜
        if not search_entire_database and folder_path != self.last_folder:
            print(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å¤¹å˜æ›´: {self.last_folder} -> {folder_path}")
            self.clear_search_cache("æ–‡ä»¶å¤¹å˜æ›´")
            self.history.clear()  # æ¸…ç©ºå†å²è®°å½•ï¼Œå› ä¸ºæ–‡ä»¶å¤¹å˜äº†
        
        if not search_entire_database:
            with self.db_lock:
                self.cursor.execute("SELECT COUNT(*) FROM file_cache WHERE folder_path = ?", (folder_path,))
                existing_count = self.cursor.fetchone()[0]
                self.cursor.execute("SELECT COUNT(*) FROM file_cache")
                total_count = self.cursor.fetchone()[0]
            
            print(f"ğŸ“Š æ•°æ®åº“çŠ¶æ€: è¯¥æ–‡ä»¶å¤¹å·²ç¼“å­˜ {existing_count} ä¸ªæ–‡ä»¶ï¼Œæ•°æ®åº“æ€»è®¡ {total_count} ä¸ªæ–‡ä»¶")
            
            if existing_count == 0 and total_count > 0:
                with self.db_lock:
                    self.cursor.execute("SELECT DISTINCT folder_path FROM file_cache LIMIT 5")
                    sample_folders = [row[0] for row in self.cursor.fetchall()]
                print(f"ğŸ”§ æ•°æ®åº“ä¸­çš„æ–‡ä»¶å¤¹ç¤ºä¾‹: {sample_folders}")
                print(f"ğŸ”§ å½“å‰æŸ¥è¯¢æ–‡ä»¶å¤¹: {folder_path}")
            
            # å¦‚æœè¯¥æ–‡ä»¶å¤¹åœ¨æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼Œæ¸…ç©ºç¼“å­˜
            if existing_count == 0:
                self.clear_search_cache("æ–‡ä»¶å¤¹æ— ç¼“å­˜è®°å½•")
                if total_count == 0:
                    error_msg = f"âŒ æ•°æ®åº“ä¸ºç©ºï¼è¯·ç­‰å¾…åå°æ„å»ºå®Œæˆæˆ–æ£€æŸ¥æ–‡ä»¶å¤¹è·¯å¾„: {folder_path}"
                else:
                    error_msg = f"âŒ æ–‡ä»¶å¤¹ {folder_path} åœ¨æ•°æ®åº“ä¸­æ²¡æœ‰ç¼“å­˜è®°å½•ï¼\nè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç­‰å¾…åå°æ„å»ºå®Œæˆã€‚"
                print(error_msg)
                # ä¸ç›´æ¥è¿”å›é”™è¯¯ï¼Œè®©åå°æ„å»ºæœ‰æœºä¼šè¿è¡Œ
        
        # åªæœ‰åœ¨æŒ‡å®šäº†folder_pathä¸”ä¸æ˜¯æœç´¢æ•´ä¸ªæ•°æ®åº“çš„æƒ…å†µä¸‹æ‰è¿›è¡Œåå°æ„å»º
        if not search_entire_database and folder_path:
            if folder_path != self.last_folder or force_cache_update:
                # å¼€å§‹æ–°çš„æ„å»ºæ—¶æ¸…ç©ºç¼“å­˜
                if folder_path != self.last_folder:
                    self.clear_search_cache("å¼€å§‹æ–°æ–‡ä»¶å¤¹æ„å»º")
                elif force_cache_update:
                    self.clear_search_cache("å¼ºåˆ¶æ›´æ–°ç¼“å­˜")
                self.start_background_building(folder_path, image_extensions, generate_missing_txt)
                self.last_folder = folder_path
            
            if self.is_building:
                print("ğŸ”„ åå°æ­£åœ¨æ„å»ºæ•°æ®åº“ï¼Œä»ç°æœ‰æ•°æ®åº“ä¸­æœç´¢...")
            else:
                print("âœ… æ•°æ®åº“å·²æ˜¯æœ€æ–°çŠ¶æ€")

        # è¶…é«˜é€Ÿæ¨¡å¼ï¼šç›´æ¥ä»æœç´¢ç»“æœä¸­é€‰æ‹©ï¼Œé¿å…é¢„è¿‡æ»¤
        if search_entire_database:
            print(f"ğŸŒ ä»æ•´ä¸ªæ•°æ®åº“ä¸­æœç´¢...")
        else:
            print(f"ğŸ“ ä»æ–‡ä»¶å¤¹ {folder_path} çš„ç°æœ‰ç¼“å­˜ä¸­æœç´¢...")
        
        keyword_list = [kw.strip().lower() for kw in keywords.split(",") if kw.strip()]
        if keyword_list:
            if strict_match:
                print(f"ğŸ¯ ä½¿ç”¨ä¸¥æ ¼åŒ¹é…æ¨¡å¼æœç´¢å…³é”®è¯: {', '.join(keyword_list)}")
            else:
                print(f"ğŸ” ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ¨¡å¼æœç´¢å…³é”®è¯: {', '.join(keyword_list)}")
        
        # è·å–æœç´¢æ¡ä»¶æŒ‡çº¹å’Œå†å²è®°å½•
        current_conditions = self.get_search_fingerprint(folder_path, keywords, negative_keywords, strict_match, search_entire_database)
        
        # é¡ºåºè¯»å–æ¨¡å¼å¤„ç†
        if sequential_read:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®é¡ºåºè¯»å–æ¨¡å¼
            if self.should_reset_sequential_mode(folder_path, keywords, negative_keywords, strict_match, search_entire_database):
                self.reset_sequential_mode("æœç´¢æ¡ä»¶æˆ–æ–‡ä»¶å¤¹å˜åŒ–")
                print("ğŸ”„ é¡ºåºè¯»å–æ¨¡å¼å·²é‡ç½®")
            
            # å¦‚æœé¡ºåºè¯»å–æ¨¡å¼æœªæ¿€æ´»ï¼Œéœ€è¦åˆå§‹åŒ–
            if not self.sequential_mode:
                print("ğŸ“– åˆå§‹åŒ–é¡ºåºè¯»å–æ¨¡å¼...")
                results = self.fast_query(folder_path, keywords, negative_keywords, strict_match, search_entire_database, [])
                
                if not results:
                    if not search_entire_database and self.is_building:
                        no_result_msg = "ğŸ’¡ åå°æ­£åœ¨æ„å»ºæ•°æ®åº“ï¼Œç¨åå¯èƒ½æœ‰æ›´å¤šç»“æœã€‚è¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ã€‚"
                        print(no_result_msg)
                        return (no_result_msg, self.blank_image)
                    else:
                        no_result_msg = "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœã€‚è¯·æ£€æŸ¥å…³é”®è¯æˆ–æ–‡ä»¶å¤¹è·¯å¾„ã€‚"
                        print(no_result_msg)
                        return (no_result_msg, self.blank_image)
                
                # åˆå§‹åŒ–é¡ºåºè¯»å–
                self.sequential_mode = True
                self.sequential_results = results
                self.sequential_index = 0
                self.sequential_folder = folder_path if not search_entire_database else ""
                self.sequential_conditions = current_conditions
                
                print(f"ğŸ“– é¡ºåºè¯»å–æ¨¡å¼å·²åˆå§‹åŒ–ï¼Œå…± {len(results)} ä¸ªç»“æœ")
            
            # è·å–ä¸‹ä¸€ä¸ªé¡ºåºç»“æœ
            selected = self.get_sequential_result()
            if not selected:
                no_result_msg = "âŒ é¡ºåºè¯»å–æ¨¡å¼å¼‚å¸¸ï¼Œæ²¡æœ‰å¯ç”¨ç»“æœ"
                print(no_result_msg)
                return (no_result_msg, self.blank_image)
            
            print(f"ğŸ“– é¡ºåºè¯»å–æ¨¡å¼: ç¬¬ {self.sequential_index}/{len(self.sequential_results)} ä¸ªç»“æœ")
            
        else:
            # éšæœºæ¨¡å¼å¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            recent_history_set = set(self.history[-history_depth:])
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜ç»“æœ - ä½¿ç”¨æ–°çš„ç¼“å­˜åˆ¤æ–­é€»è¾‘
            if (self.should_use_cache() and 
                self.last_search_conditions == current_conditions and 
                self.last_search_results is not None):
                print("âš¡ æœç´¢æ¡ä»¶æœªå˜åŒ–ä¸”æ•°æ®åº“ç¨³å®šï¼Œä½¿ç”¨ç¼“å­˜ç»“æœç›´æ¥é€‰æ‹©")
                
                # æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶
                if len(self.last_search_results) == 1:
                    print("ğŸ“Œ åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶ï¼Œè·³è¿‡history_depthé™åˆ¶")
                    selected = self.last_search_results[0]
                else:
                    selected = self.ultra_fast_select(self.last_search_results, recent_history_set)
            else:
                # æœç´¢æ¡ä»¶å˜åŒ–æˆ–ä¸èƒ½ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°æŸ¥è¯¢
                if self.is_building:
                    print("ğŸ” æ•°æ®åº“æ­£åœ¨æ„å»ºä¸­ï¼Œé‡æ–°æŸ¥è¯¢ç°æœ‰æ•°æ®...")
                else:
                    print("ğŸ” æœç´¢æ¡ä»¶å·²å˜åŒ–æˆ–ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°æŸ¥è¯¢æ•°æ®åº“...")
                
                results = self.fast_query(folder_path, keywords, negative_keywords, strict_match, search_entire_database, [])
                
                # åªæœ‰åœ¨æ•°æ®åº“ä¸åœ¨æ„å»ºæ—¶æ‰ç¼“å­˜ç»“æœ
                if not self.is_building:
                    self.last_search_conditions = current_conditions
                    self.last_search_results = results
                    print(f"ğŸ’¾ å·²ç¼“å­˜æœç´¢ç»“æœ ({len(results)} ä¸ª)")
                else:
                    print(f"âš ï¸ æ•°æ®åº“æ„å»ºä¸­ï¼Œä¸ç¼“å­˜æœç´¢ç»“æœ ({len(results)} ä¸ª)")
                
                print(f"ğŸ¯ æ•°æ®åº“æŸ¥è¯¢åˆ° {len(results)} ä¸ªç»“æœ")
                
                # æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶
                if len(results) == 1:
                    print("ğŸ“Œ åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶ï¼Œè·³è¿‡history_depthé™åˆ¶")
                    selected = results[0]
                else:
                    selected = self.ultra_fast_select(results, recent_history_set)
            
            # å¦‚æœæ²¡æœ‰é€‰ä¸­ç»“æœï¼Œæ¸…ç©ºå†å²è®°å½•é‡è¯•
            if not selected and self.history:
                print("ğŸ”„ æ²¡æœ‰å¯ç”¨ç»“æœï¼Œæ¸…ç©ºå†å²è®°å½•é‡è¯•...")
                self.history.clear()
                recent_history_set = set()
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶
                if self.last_search_results and len(self.last_search_results) == 1:
                    print("ğŸ“Œ é‡è¯•æ—¶å‘ç°åªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶ï¼Œè·³è¿‡history_depthé™åˆ¶")
                    selected = self.last_search_results[0]
                else:
                    selected = self.ultra_fast_select(self.last_search_results, recent_history_set)
            
            # å¤„ç†æœ€ç»ˆç»“æœ
            if not selected:
                if not search_entire_database and self.is_building:
                    no_result_msg = "ğŸ’¡ åå°æ­£åœ¨æ„å»ºæ•°æ®åº“ï¼Œç¨åå¯èƒ½æœ‰æ›´å¤šç»“æœã€‚è¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ã€‚"
                    print(no_result_msg)
                    return (no_result_msg, self.blank_image)
                else:
                    no_result_msg = "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœã€‚è¯·æ£€æŸ¥å…³é”®è¯æˆ–æ–‡ä»¶å¤¹è·¯å¾„ã€‚"
                    print(no_result_msg)
                    return (no_result_msg, self.blank_image)
            
            # éšæœºæ¨¡å¼ä¸‹æ›´æ–°å†å²è®°å½•
            txt_path = selected[0]
            
            # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ç¬¦åˆæ¡ä»¶ï¼Œä¸æ›´æ–°å†å²è®°å½•ï¼Œç¡®ä¿ä¸‹æ¬¡ä»ç„¶èƒ½é€‰æ‹©åˆ°å®ƒ
            if self.last_search_results and len(self.last_search_results) == 1:
                print("ğŸ“Œ åªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œä¸æ›´æ–°å†å²è®°å½•ä»¥ç¡®ä¿å¯é‡å¤é€‰æ‹©")
            else:
                self.history.append(txt_path)
                if len(self.history) > history_depth * 2:
                    self.history = self.history[-history_depth:]
        
        # å¤„ç†é€‰ä¸­çš„ç»“æœ
        txt_path, content, image_path = selected
        
        if not sequential_read:
            if search_entire_database:
                print(f"ğŸ“„ é€‰ä¸­æ–‡ä»¶: {txt_path}")
        
        image_tensor = self.blank_image
        if image_path and os.path.exists(image_path):
            try:
                img = Image.open(image_path)
                img = img.convert("RGB")
                img_array = np.array(img).astype(np.float32) / 255.0
                image_tensor = torch.from_numpy(img_array)[None,]
            except Exception as e:
                print(f"âš ï¸ å›¾ç‰‡åŠ è½½é”™è¯¯ [{image_path}]: {str(e)}ï¼Œä½¿ç”¨ç©ºç™½å›¾ç‰‡")
                # å›¾ç‰‡åŠ è½½å¤±è´¥æ—¶ä¿æŒä½¿ç”¨ç©ºç™½å›¾ç‰‡
        else:
            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæ•°æ®åº“ä¸­çš„è®°å½•ä¿è¯äº†å›¾ç‰‡å­˜åœ¨
            print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ [{image_path}]ï¼Œä½¿ç”¨ç©ºç™½å›¾ç‰‡")
        
        return (content, image_tensor)
    
    def __del__(self):
        self.stop_background_building()
        # æ¸…ç†é¡ºåºè¯»å–æ¨¡å¼
        if hasattr(self, 'sequential_mode') and self.sequential_mode:
            self.reset_sequential_mode("å¯¹è±¡é”€æ¯")
        if self.conn:
            with self.db_lock:
                self.conn.close()

class ImageBatchReader:
    def __init__(self):
        self.blank_image = self.create_blank_image()
        
        # é¡ºåºè¯»å–ç›¸å…³å±æ€§
        self.sequential_mode = False
        self.sequential_results = []
        self.sequential_index = 0
        self.sequential_folder = ""
        self.last_folder = ""
        self.last_extensions = ""
        
        print("ğŸ–¼ï¸ å›¾ç‰‡æ‰¹é‡è¯»å–å™¨å·²åˆå§‹åŒ–")
    
    def create_blank_image(self):
        """åˆ›å»ºç©ºç™½å›¾ç‰‡"""
        blank = Image.new("RGB", (512, 512), (0, 0, 0))
        blank_array = np.array(blank).astype(np.float32) / 255.0
        return torch.from_numpy(blank_array)[None,]
    
    def load_image_safe(self, image_path):
        """å®‰å…¨åŠ è½½å›¾ç‰‡"""
        try:
            img = Image.open(image_path)
            img = img.convert("RGB")
            img_array = np.array(img).astype(np.float32) / 255.0
            return torch.from_numpy(img_array)[None,]
        except Exception as e:
            print(f"âš ï¸ å›¾ç‰‡åŠ è½½é”™è¯¯ [{image_path}]: {str(e)}ï¼Œä½¿ç”¨ç©ºç™½å›¾ç‰‡")
            return self.blank_image
    
    def reset_sequential_mode(self, reason=""):
        """é‡ç½®é¡ºåºè¯»å–æ¨¡å¼"""
        if reason:
            print(f"ğŸ”„ é‡ç½®é¡ºåºè¯»å–æ¨¡å¼: {reason}")
        self.sequential_mode = False
        self.sequential_results = []
        self.sequential_index = 0
        self.sequential_folder = ""
    
    def should_reset_sequential_mode(self, folder_path, extensions):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®é¡ºåºè¯»å–æ¨¡å¼"""
        # å¦‚æœæ–‡ä»¶å¤¹å˜åŒ–ï¼Œéœ€è¦é‡ç½®
        if self.sequential_folder != folder_path:
            return True
        
        # å¦‚æœæ‰©å±•åå˜åŒ–ï¼Œéœ€è¦é‡ç½®
        if self.last_extensions != extensions:
            return True
        
        return False
    
    def get_image_files(self, folder_path, extensions):
        """è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶"""
        if not os.path.exists(folder_path):
            return []
        
        image_files = []
        ext_list = [ext.strip().lower() for ext in extensions.split(',')]
        
        for root, _, files in os.walk(folder_path):
            for file in files:
                if any(file.lower().endswith(f'.{ext}') for ext in ext_list):
                    image_files.append(os.path.join(root, file))
        
        return sorted(image_files)  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´
    
    def get_sequential_batch(self, batch_size):
        """è·å–ä¸‹ä¸€æ‰¹é¡ºåºç»“æœ"""
        if not self.sequential_results:
            return []
        
        batch = []
        for _ in range(batch_size):
            if self.sequential_index >= len(self.sequential_results):
                # åˆ°è¾¾æœ«å°¾ï¼Œå¾ªç¯å›åˆ°å¼€å§‹
                self.sequential_index = 0
                print(f"ğŸ”„ é¡ºåºè¯»å–å·²åˆ°è¾¾æœ«å°¾ï¼Œé‡æ–°å¼€å§‹å¾ªç¯ (å…± {len(self.sequential_results)} ä¸ªå›¾ç‰‡)")
            
            if self.sequential_results:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                batch.append(self.sequential_results[self.sequential_index])
                self.sequential_index += 1
        
        return batch
    
    def get_random_batch(self, image_files, batch_size, seed):
        """è·å–éšæœºæ‰¹æ¬¡"""
        if not image_files:
            return []
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        
        # å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæˆ–ç­‰äºæ€»æ–‡ä»¶æ•°ï¼Œè¿”å›æ‰“ä¹±åçš„æ‰€æœ‰æ–‡ä»¶
        if batch_size >= len(image_files):
            shuffled_files = image_files.copy()
            random.shuffle(shuffled_files)
            return shuffled_files
        
        return random.sample(image_files, batch_size)
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "folder_path": ("STRING", {"default": ""}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 50}),
                "image_extensions": ("STRING", {"default": "png,jpg,jpeg,webp,bmp,gif,tiff"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
            },
            "optional": {
                "sequential_read": ("BOOLEAN", {"default": False}),
                "shuffle_on_reset": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    OUTPUT_IS_LIST = (True,)
    FUNCTION = "process"
    CATEGORY = "hhy"

    def process(self, folder_path, batch_size, image_extensions, seed, sequential_read=False, shuffle_on_reset=False):
        
        # è¾“å…¥éªŒè¯
        if not folder_path or not folder_path.strip():
            error_msg = "âŒ è¯·æŒ‡å®šå›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„"
            print(error_msg)
            return ([self.blank_image],)
        
        folder_path = os.path.normpath(folder_path.strip())
        
        if not os.path.exists(folder_path):
            error_msg = f"âŒ è·¯å¾„ä¸å­˜åœ¨: {folder_path}"
            print(error_msg)
            return ([self.blank_image],)
        
        if not os.path.isdir(folder_path):
            error_msg = f"âŒ è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}"
            print(error_msg)
            return ([self.blank_image],)
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        print(f"ğŸ“ æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        image_files = self.get_image_files(folder_path, image_extensions)
        
        if not image_files:
            error_msg = f"âŒ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶ (æ”¯æŒæ ¼å¼: {image_extensions})"
            print(error_msg)
            return ([self.blank_image],)
        
        print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        
        # é™åˆ¶æ‰¹æ¬¡å¤§å°ä¸è¶…è¿‡å¯ç”¨å›¾ç‰‡æ•°é‡
        original_batch_size = batch_size
        max_available = len(image_files)
        batch_size = min(batch_size, max_available)
        
        if original_batch_size > max_available:
            print(f"âš ï¸ æ‰¹æ¬¡å¤§å° ({original_batch_size}) è¶…è¿‡å¯ç”¨å›¾ç‰‡æ•°é‡ ({max_available})ï¼Œå·²è°ƒæ•´ä¸º {batch_size}")
        
        # é¡ºåºè¯»å–æ¨¡å¼å¤„ç†
        if sequential_read:
            print("ğŸ“– é¡ºåºè¯»å–æ¨¡å¼å·²å¯ç”¨")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®é¡ºåºè¯»å–æ¨¡å¼
            if self.should_reset_sequential_mode(folder_path, image_extensions):
                self.reset_sequential_mode("æ–‡ä»¶å¤¹æˆ–æ‰©å±•åå˜åŒ–")
            
            # å¦‚æœé¡ºåºè¯»å–æ¨¡å¼æœªæ¿€æ´»ï¼Œéœ€è¦åˆå§‹åŒ–
            if not self.sequential_mode:
                print("ğŸ“– åˆå§‹åŒ–é¡ºåºè¯»å–æ¨¡å¼...")
                
                # åˆå§‹åŒ–é¡ºåºè¯»å–
                self.sequential_mode = True
                self.sequential_results = image_files.copy()
                
                # å¦‚æœå¯ç”¨äº†é‡ç½®æ—¶æ‰“ä¹±ï¼Œåˆ™æ‰“ä¹±æ–‡ä»¶åˆ—è¡¨
                if shuffle_on_reset:
                    random.seed(seed)
                    random.shuffle(self.sequential_results)
                    print("ğŸ² å·²æ‰“ä¹±æ–‡ä»¶é¡ºåº")
                
                self.sequential_index = 0
                self.sequential_folder = folder_path
                self.last_extensions = image_extensions
                
                print(f"ğŸ“– é¡ºåºè¯»å–æ¨¡å¼å·²åˆå§‹åŒ–ï¼Œå…± {len(self.sequential_results)} ä¸ªå›¾ç‰‡")
            
            # è·å–ä¸‹ä¸€æ‰¹é¡ºåºç»“æœ
            selected_files = self.get_sequential_batch(batch_size)
            current_start = self.sequential_index - len(selected_files)
            if current_start < 0:
                current_start = len(self.sequential_results) + current_start
            
            print(f"ğŸ“– é¡ºåºè¯»å–: ç¬¬ {current_start + 1}-{current_start + len(selected_files)} ä¸ªå›¾ç‰‡ (å…± {len(self.sequential_results)} ä¸ª)")
            
        else:
            # éšæœºæ¨¡å¼å¤„ç†
            print("ğŸ² éšæœºè¯»å–æ¨¡å¼")
            selected_files = self.get_random_batch(image_files, batch_size, seed)
            print(f"ğŸ¯ éšæœºé€‰æ‹©äº† {len(selected_files)} ä¸ªå›¾ç‰‡")
        
        # åŠ è½½é€‰ä¸­çš„å›¾ç‰‡
        image_tensors = []
        for i, image_path in enumerate(selected_files):
            print(f"ğŸ“„ åŠ è½½å›¾ç‰‡ {i+1}/{len(selected_files)}: {os.path.basename(image_path)}")
            image_tensor = self.load_image_safe(image_path)
            image_tensors.append(image_tensor)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(image_tensors)} ä¸ªå›¾ç‰‡")
        
        return (image_tensors,)

NODE_CLASS_MAPPINGS = {
    "TextWithImageReader": TextWithImageReader,
    "ImageBatchReader": ImageBatchReader
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "TextWithImageReader": "Text-Image Pair Reader",
    "ImageBatchReader": "Image Batch Reader"
}